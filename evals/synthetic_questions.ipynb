<<<<<<< HEAD
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Bacteriophage targeting of gut bacterium atten...\n",
       "1      Chronic liver disease due to alcohol-use disor...\n",
       "2      and liver injury. Compared with non-alcoholic ...\n",
       "3      bacteriophages that target cytolytic E. faecal...\n",
       "4      The most severe form of alcohol-related liver ...\n",
       "                             ...                        \n",
       "199    project and provided validation; and C.K.S.-T....\n",
       "200    Ferring Pharmaceuticals; is an inventor on pat...\n",
       "201    Lactose drives Enterococcus expansion to promo...\n",
       "202    Allogeneic hematopoietic cell transplantation ...\n",
       "203    provide a substrate for Enterococcus growth, a...\n",
       "Name: text, Length: 204, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lancedb\n",
    "import pandas as pd\n",
    "import random\n",
    "from lancedb.table import Table\n",
    "\n",
    "# pd.set_option(\"display.max_colwidth\", 160)\n",
    "\n",
    "# db = lancedb.connect(rf\"C:\\Users\\vtorr\\Work\\Projects\\aipatent\\app\\data\\lancedb\")\n",
    "# document_table = db.open_table(\"document\")\n",
    "\n",
    "# def rows_to_df(table: Table) -> pd.DataFrame:\n",
    "#     return table.search(\" \").limit(999999).to_pandas()\n",
    "\n",
    "# sample_document = rows_to_df(document_table)\n",
    "# sample_document.text\n",
    "\n",
    "sample_document = pd.read_csv(r\"C:\\Users\\vtorr\\Work\\Projects\\aipatent\\evals\\GVHD-ALD-papers-not-false.csv\")\n",
    "sample_document.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Document(BaseModel):\n",
    "    title: str\n",
    "    filename: str\n",
    "    id: str\n",
    "    text: str\n",
    "\n",
    "sample_chunks = [\n",
    "    Document(\n",
    "        title=\"Lactose drives Enterococcus expansion to promote graft-versus-host disease\",\n",
    "        filename=row[\"filename\"],\n",
    "        id=str(row[\"chunk_id\"]),\n",
    "        text=row[\"text\"]\n",
    "    )\n",
    "    for _, row in sample_document.iterrows()\n",
    "]\n",
    "\n",
    "n_questions = 3\n",
    "\n",
    "example_questions = [\n",
    "    \"What is the composition of the substance?\",\n",
    "    \"How does the component operate, and what are his effects in the organism?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChunkEval(question='What targeting strategy is described for modulating gut bacteria in the paper?', answer='The paper describes a strategy where bacteriophages are used to specifically target and deplete gut bacteria, which in turn helps in attenuating alcoholic liver disease.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: What targeting strategy is described for modulating gut bacteria in the paper?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n'),\n",
       " ChunkEval(question='How does the bacteriophage intervention affect the progression of alcoholic liver disease?', answer='By targeting specific gut bacteria, the bacteriophage intervention reduces bacterial populations that contribute to disease progression, thereby attenuating alcoholic liver disease.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: How does the bacteriophage intervention affect the progression of alcoholic liver disease?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n'),\n",
       " ChunkEval(question='What therapeutic potential does bacteriophage targeting of gut bacteria offer according to the study?', answer='The study implies that precise bacteriophage targeting of gut bacteria can modulate the gut-liver axis, offering a potential therapeutic approach to mitigate alcoholic liver disease.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: What therapeutic potential does bacteriophage targeting of gut bacteria offer according to the study?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import List\n",
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class ChunkEval(QuestionAnswer):\n",
    "    chunk_id: str\n",
    "    question_with_context: str\n",
    "\n",
    "async def generate_evals(\n",
    "    document: Document, n_questions: int, example_questions: List[str]\n",
    ") -> List[ChunkEval]:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Generate `{n_questions}` question-answer pairs about {document.title}. The answers should primarily be derived from information in this piece of text from a microbiology paper:\n",
    "\n",
    "        <content>\n",
    "        {document.text}\n",
    "        </content>\n",
    "\n",
    "        Example questions:\n",
    "        {chr(10).join(f'- {q}' for q in example_questions)}\n",
    "\n",
    "        Provide a concise and specific answer for each question.\n",
    "        Do not use the exact example questions. Use them only as inspiration for the types of more specific questions to generate.\n",
    "        Do not include answers that are not in the content.\n",
    "        Questions should ask about biological characteristics and answers should refer to the content for clarification on the composition, effect or method of action of a given component or mechanism described in the piece of text.\n",
    "        Stylistically, the questions should resemble what would be asked to a RAG-based answer tool to access relevant information about target antigens and disease drivers to enrich a patent generation process.\n",
    "        \"\"\"\n",
    "\n",
    "    def make_context(question: str) -> str:\n",
    "        return f\"\"\"You receive the following question:\n",
    "Question: {question}\n",
    "This is about the following piece of text from a microbiology paper:\n",
    "Paper Title: {document.title}\n",
    "Paper chunk: {document.text}\n",
    "\"\"\"\n",
    "    try:\n",
    "        pairs = client.chat.completions.create_iterable(\n",
    "            model=\"o3-mini\",\n",
    "            response_model=QuestionAnswer,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            ChunkEval(\n",
    "                question=pair.question,\n",
    "                answer=pair.answer,\n",
    "                chunk_id=document.id,\n",
    "                question_with_context=make_context(pair.question),\n",
    "            )\n",
    "            async for pair in pairs\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating evals: {str(e)}\")\n",
    "        return []\n",
    "    \n",
    "first_chunk_res = await generate_evals(sample_chunks[0], n_questions, example_questions)\n",
    "first_chunk_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChunkEval(question='What component is used to target gut bacteria, and what impact does this have on the disease state according to the study?', answer='The study uses bacteriophage therapy to selectively target gut bacteria, which results in the attenuation of alcoholic liver disease.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: What component is used to target gut bacteria, and what impact does this have on the disease state according to the study?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n'),\n",
       " ChunkEval(question='How does the bacteriophage mechanism described in the paper modify gut bacterial populations?', answer='The bacteriophage specifically targets a gut bacterium, reducing its influence and thereby contributing to the attenuation of alcoholic liver disease.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: How does the bacteriophage mechanism described in the paper modify gut bacterial populations?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n'),\n",
       " ChunkEval(question='What role does bacteriophage play in the modulation of pathogenic processes in the context of the reported study?', answer='Bacteriophage is used to target gut bacteria, and this targeted approach leads to the attenuation of alcoholic liver disease by altering pathogenic processes.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: What role does bacteriophage play in the modulation of pathogenic processes in the context of the reported study?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "class ChunkProcessingError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "async def process_chunk(\n",
    "    document: Document,\n",
    "    n_questions: int,\n",
    "    example_questions: List[str],\n",
    "    semaphore: asyncio.Semaphore,\n",
    ") -> List[ChunkEval]:\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            return await generate_evals(document, n_questions, example_questions)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing chunk {document.id}: {str(e)}\")\n",
    "            raise ChunkProcessingError(f\"Failed to process chunk {document.id}\") from e\n",
    "\n",
    "\n",
    "# Test that we get the same results as directly calling generate_evals\n",
    "await process_chunk(\n",
    "    sample_chunks[0], n_questions, example_questions, asyncio.Semaphore(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID 257 is out of bounds.\n",
      "Chunk ID 253 is out of bounds.\n",
      "Chunk ID 264 is out of bounds.\n",
      "Chunk ID 254 is out of bounds.\n",
      "Chunk ID 238 is out of bounds.\n",
      "Chunk ID 270 is out of bounds.\n",
      "Chunk ID 225 is out of bounds.\n",
      "Chunk ID 368 is out of bounds.\n",
      "Chunk ID 375 is out of bounds.\n",
      "Chunk ID 276 is out of bounds.\n",
      "Chunk ID 222 is out of bounds.\n",
      "Chunk ID 264 is out of bounds.\n",
      "Chunk ID 217 is out of bounds.\n",
      "Chunk ID 700 is out of bounds.\n",
      "Chunk ID 244 is out of bounds.\n",
      "Chunk ID 689 is out of bounds.\n",
      "Chunk ID 514 is out of bounds.\n",
      "Chunk ID 240 is out of bounds.\n",
      "Chunk ID 228 is out of bounds.\n",
      "Chunk ID 214 is out of bounds.\n",
      "Chunk ID 673 is out of bounds.\n",
      "Chunk ID 693 is out of bounds.\n",
      "Chunk ID 463 is out of bounds.\n",
      "Chunk ID 229 is out of bounds.\n",
      "Chunk ID 658 is out of bounds.\n",
      "Chunk ID 501 is out of bounds.\n",
      "Chunk ID 425 is out of bounds.\n",
      "Chunk ID 700 is out of bounds.\n",
      "Chunk ID 322 is out of bounds.\n",
      "Chunk ID 697 is out of bounds.\n",
      "Chunk ID 484 is out of bounds.\n",
      "Chunk ID 261 is out of bounds.\n",
      "Chunk ID 230 is out of bounds.\n",
      "Chunk ID 292 is out of bounds.\n",
      "Chunk ID 451 is out of bounds.\n",
      "Chunk ID 692 is out of bounds.\n",
      "Chunk ID 410 is out of bounds.\n",
      "Chunk ID 547 is out of bounds.\n",
      "Chunk ID 227 is out of bounds.\n",
      "Chunk ID 563 is out of bounds.\n",
      "Chunk ID 396 is out of bounds.\n",
      "Chunk ID 241 is out of bounds.\n",
      "Chunk ID 481 is out of bounds.\n",
      "Chunk ID 298 is out of bounds.\n",
      "Chunk ID 543 is out of bounds.\n",
      "Chunk ID 267 is out of bounds.\n",
      "Chunk ID 206 is out of bounds.\n",
      "Chunk ID 490 is out of bounds.\n",
      "Chunk ID 692 is out of bounds.\n",
      "Chunk ID 691 is out of bounds.\n",
      "Chunk ID 699 is out of bounds.\n",
      "Chunk ID 397 is out of bounds.\n",
      "Chunk ID 708 is out of bounds.\n",
      "Chunk ID 509 is out of bounds.\n",
      "Chunk ID 577 is out of bounds.\n",
      "Chunk ID 691 is out of bounds.\n",
      "Chunk ID 707 is out of bounds.\n",
      "Chunk ID 550 is out of bounds.\n",
      "Chunk ID 220 is out of bounds.\n",
      "Chunk ID 514 is out of bounds.\n",
      "Chunk ID 308 is out of bounds.\n",
      "Chunk ID 371 is out of bounds.\n",
      "Chunk ID 651 is out of bounds.\n",
      "Chunk ID 215 is out of bounds.\n",
      "Chunk ID 425 is out of bounds.\n",
      "Chunk ID 224 is out of bounds.\n",
      "Chunk ID 410 is out of bounds.\n",
      "Chunk ID 466 is out of bounds.\n",
      "Chunk ID 262 is out of bounds.\n",
      "Chunk ID 451 is out of bounds.\n",
      "Chunk ID 706 is out of bounds.\n",
      "Chunk ID 546 is out of bounds.\n",
      "Chunk ID 492 is out of bounds.\n",
      "Chunk ID 561 is out of bounds.\n",
      "Chunk ID 234 is out of bounds.\n",
      "Chunk ID 227 is out of bounds.\n",
      "Chunk ID 515 is out of bounds.\n",
      "Chunk ID 501 is out of bounds.\n",
      "Chunk ID 370 is out of bounds.\n",
      "Chunk ID 278 is out of bounds.\n",
      "Chunk ID 577 is out of bounds.\n",
      "Chunk ID 694 is out of bounds.\n",
      "Chunk ID 658 is out of bounds.\n",
      "Chunk ID 238 is out of bounds.\n",
      "Chunk ID 209 is out of bounds.\n",
      "Chunk ID 258 is out of bounds.\n",
      "Chunk ID 245 is out of bounds.\n",
      "Chunk ID 414 is out of bounds.\n",
      "Chunk ID 245 is out of bounds.\n",
      "Chunk ID 702 is out of bounds.\n",
      "Chunk ID 412 is out of bounds.\n",
      "Chunk ID 447 is out of bounds.\n",
      "Chunk ID 725 is out of bounds.\n",
      "Chunk ID 530 is out of bounds.\n",
      "Chunk ID 214 is out of bounds.\n",
      "Chunk ID 277 is out of bounds.\n",
      "Chunk ID 278 is out of bounds.\n",
      "Chunk ID 484 is out of bounds.\n",
      "Chunk ID 273 is out of bounds.\n",
      "Chunk ID 240 is out of bounds.\n",
      "Chunk ID 263 is out of bounds.\n",
      "Chunk ID 664 is out of bounds.\n",
      "Chunk ID 728 is out of bounds.\n",
      "Chunk ID 262 is out of bounds.\n",
      "Chunk ID 607 is out of bounds.\n",
      "Chunk ID 298 is out of bounds.\n",
      "Chunk ID 247 is out of bounds.\n",
      "Chunk ID 549 is out of bounds.\n",
      "Chunk ID 540 is out of bounds.\n",
      "Chunk ID 264 is out of bounds.\n",
      "Chunk ID 520 is out of bounds.\n",
      "Chunk ID 702 is out of bounds.\n",
      "Chunk ID 229 is out of bounds.\n",
      "Chunk ID 509 is out of bounds.\n",
      "Chunk ID 277 is out of bounds.\n",
      "Chunk ID 263 is out of bounds.\n",
      "Chunk ID 424 is out of bounds.\n",
      "Chunk ID 283 is out of bounds.\n",
      "Chunk ID 410 is out of bounds.\n",
      "Chunk ID 515 is out of bounds.\n",
      "Chunk ID 597 is out of bounds.\n",
      "Chunk ID 266 is out of bounds.\n",
      "Chunk ID 561 is out of bounds.\n",
      "Chunk ID 704 is out of bounds.\n",
      "Chunk ID 217 is out of bounds.\n",
      "Chunk ID 697 is out of bounds.\n",
      "Chunk ID 542 is out of bounds.\n",
      "Chunk ID 746 is out of bounds.\n",
      "Chunk ID 491 is out of bounds.\n",
      "Chunk ID 261 is out of bounds.\n",
      "Chunk ID 396 is out of bounds.\n",
      "Chunk ID 689 is out of bounds.\n",
      "Chunk ID 223 is out of bounds.\n",
      "Chunk ID 244 is out of bounds.\n",
      "Chunk ID 255 is out of bounds.\n",
      "Chunk ID 740 is out of bounds.\n",
      "Chunk ID 490 is out of bounds.\n",
      "Chunk ID 691 is out of bounds.\n",
      "Chunk ID 567 is out of bounds.\n",
      "Chunk ID 252 is out of bounds.\n",
      "Chunk ID 260 is out of bounds.\n",
      "Chunk ID 262 is out of bounds.\n",
      "Chunk ID 499 is out of bounds.\n",
      "Chunk ID 514 is out of bounds.\n",
      "Chunk ID 252 is out of bounds.\n",
      "Chunk ID 254 is out of bounds.\n",
      "Chunk ID 543 is out of bounds.\n",
      "Chunk ID 224 is out of bounds.\n",
      "Chunk ID 220 is out of bounds.\n",
      "Chunk ID 245 is out of bounds.\n",
      "Chunk ID 562 is out of bounds.\n",
      "Chunk ID 725 is out of bounds.\n",
      "Chunk ID 619 is out of bounds.\n",
      "Chunk ID 237 is out of bounds.\n",
      "Chunk ID 542 is out of bounds.\n",
      "Chunk ID 621 is out of bounds.\n",
      "Chunk ID 563 is out of bounds.\n",
      "Chunk ID 447 is out of bounds.\n",
      "Chunk ID 562 is out of bounds.\n",
      "Chunk ID 549 is out of bounds.\n",
      "Chunk ID 745 is out of bounds.\n",
      "Chunk ID 567 is out of bounds.\n",
      "Chunk ID 708 is out of bounds.\n",
      "Chunk ID 244 is out of bounds.\n",
      "Chunk ID 530 is out of bounds.\n",
      "Chunk ID 214 is out of bounds.\n",
      "Chunk ID 256 is out of bounds.\n",
      "Chunk ID 472 is out of bounds.\n",
      "Chunk ID 209 is out of bounds.\n",
      "Chunk ID 228 is out of bounds.\n",
      "Chunk ID 447 is out of bounds.\n",
      "Chunk ID 277 is out of bounds.\n",
      "Chunk ID 252 is out of bounds.\n",
      "Chunk ID 259 is out of bounds.\n",
      "Chunk ID 398 is out of bounds.\n",
      "Chunk ID 368 is out of bounds.\n",
      "Chunk ID 301 is out of bounds.\n",
      "Chunk ID 276 is out of bounds.\n",
      "Chunk ID 314 is out of bounds.\n",
      "Chunk ID 238 is out of bounds.\n",
      "Chunk ID 704 is out of bounds.\n",
      "Chunk ID 664 is out of bounds.\n",
      "Chunk ID 219 is out of bounds.\n",
      "Chunk ID 567 is out of bounds.\n",
      "Chunk ID 207 is out of bounds.\n",
      "Chunk ID 212 is out of bounds.\n",
      "Chunk ID 258 is out of bounds.\n",
      "Chunk ID 597 is out of bounds.\n",
      "Chunk ID 550 is out of bounds.\n",
      "Chunk ID 466 is out of bounds.\n",
      "Chunk ID 690 is out of bounds.\n",
      "Chunk ID 690 is out of bounds.\n",
      "Chunk ID 375 is out of bounds.\n",
      "Chunk ID 207 is out of bounds.\n",
      "Chunk ID 696 is out of bounds.\n",
      "Chunk ID 566 is out of bounds.\n",
      "Chunk ID 414 is out of bounds.\n",
      "Chunk ID 479 is out of bounds.\n",
      "Chunk ID 273 is out of bounds.\n",
      "Chunk ID 479 is out of bounds.\n",
      "Chunk ID 225 is out of bounds.\n",
      "Chunk ID 539 is out of bounds.\n",
      "Chunk ID 314 is out of bounds.\n",
      "Chunk ID 280 is out of bounds.\n",
      "Chunk ID 249 is out of bounds.\n",
      "Chunk ID 481 is out of bounds.\n",
      "Chunk ID 490 is out of bounds.\n",
      "Chunk ID 566 is out of bounds.\n",
      "Chunk ID 227 is out of bounds.\n",
      "Chunk ID 397 is out of bounds.\n",
      "Chunk ID 607 is out of bounds.\n",
      "Chunk ID 527 is out of bounds.\n",
      "Chunk ID 544 is out of bounds.\n",
      "Chunk ID 692 is out of bounds.\n",
      "Chunk ID 527 is out of bounds.\n",
      "Chunk ID 472 is out of bounds.\n",
      "Chunk ID 260 is out of bounds.\n",
      "Chunk ID 577 is out of bounds.\n",
      "Chunk ID 208 is out of bounds.\n",
      "Chunk ID 258 is out of bounds.\n",
      "Chunk ID 730 is out of bounds.\n",
      "Chunk ID 540 is out of bounds.\n",
      "Chunk ID 673 is out of bounds.\n",
      "Chunk ID 281 is out of bounds.\n",
      "Chunk ID 491 is out of bounds.\n",
      "Chunk ID 220 is out of bounds.\n",
      "Generated 612 ChunkEvals.\n",
      "Saved 306 samples as 'synthetic_eval_cleaned_gvhd_ald.json'\n",
      "Saved 306 samples as 'synthetic_finetune_cleaned_gvhd_ald.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "async def create_synthetic_dataset(\n",
    "    documents: List[Document],\n",
    "    n_questions: int,\n",
    "    example_questions: List[str],\n",
    "    max_concurrency: int = 10,\n",
    ") -> List[ChunkEval]:\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "    tasks = [\n",
    "        process_chunk(document, n_questions, example_questions, semaphore)\n",
    "        for document in documents\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    dataset = []\n",
    "    for result in results:\n",
    "        if isinstance(result, ChunkProcessingError):\n",
    "            print(result)\n",
    "        elif isinstance(result, list):\n",
    "            dataset.extend(result)\n",
    "        else:\n",
    "            print(f\"Unexpected result type: {type(result)}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_eval_data(dataset: List[ChunkEval], filename: str):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump([chunk_eval.model_dump() for chunk_eval in dataset], f, indent=2)\n",
    "\n",
    "def save_ft_data(dataset: List[ChunkEval], filename: str):\n",
    "    documents_df = pd.read_csv(r\"C:\\Users\\vtorr\\Work\\Projects\\aipatent\\evals\\GVHD-ALD-papers-not-false.csv\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        for chunk_eval in dataset:\n",
    "            if int(chunk_eval.chunk_id) < len(documents_df):\n",
    "                document_text = documents_df.iloc[int(chunk_eval.chunk_id)].text\n",
    "                json_object = {\n",
    "                    'query': chunk_eval.question_with_context,\n",
    "                    'relevant_passages': [document_text],\n",
    "                }\n",
    "                f.write(json.dumps(json_object) + '\\n')\n",
    "            else:\n",
    "                print(f\"Chunk ID {chunk_eval.chunk_id} is out of bounds.\")\n",
    "\n",
    "synthetic_dataset = await create_synthetic_dataset(\n",
    "    sample_chunks, n_questions, example_questions\n",
    ")\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "random.shuffle(synthetic_dataset)\n",
    "\n",
    "# Split the dataset into two halves\n",
    "split_index = len(synthetic_dataset) // 2\n",
    "eval_dataset = synthetic_dataset[:split_index]\n",
    "finetune_dataset = synthetic_dataset[split_index:]\n",
    "\n",
    "# Save the two datasets\n",
    "save_eval_data(eval_dataset, \"synthetic_eval_cleaned_gvhd_ald.json\")\n",
    "save_ft_data(finetune_dataset, \"synthetic_finetune_cleaned_gvhd_ald.jsonl\")\n",
    "\n",
    "print(f\"Generated {len(synthetic_dataset)} ChunkEvals.\")\n",
    "print(f\"Saved {len(eval_dataset)} samples as 'synthetic_eval_cleaned_gvhd_ald.json'\")\n",
    "print(f\"Saved {len(finetune_dataset)} samples as 'synthetic_finetune_cleaned_gvhd_ald.jsonl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Bacteriophage targeting of gut bacterium atten...\n",
       "1      Chronic liver disease due to alcohol-use disor...\n",
       "2      and liver injury. Compared with non-alcoholic ...\n",
       "3      bacteriophages that target cytolytic E. faecal...\n",
       "4      The most severe form of alcohol-related liver ...\n",
       "                             ...                        \n",
       "199    project and provided validation; and C.K.S.-T....\n",
       "200    Ferring Pharmaceuticals; is an inventor on pat...\n",
       "201    Lactose drives Enterococcus expansion to promo...\n",
       "202    Allogeneic hematopoietic cell transplantation ...\n",
       "203    provide a substrate for Enterococcus growth, a...\n",
       "Name: text, Length: 204, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lancedb\n",
    "import pandas as pd\n",
    "import random\n",
    "from lancedb.table import Table\n",
    "\n",
    "# pd.set_option(\"display.max_colwidth\", 160)\n",
    "\n",
    "# db = lancedb.connect(rf\"C:\\Users\\vtorr\\Work\\Projects\\aipatent\\app\\data\\lancedb\")\n",
    "# document_table = db.open_table(\"document\")\n",
    "\n",
    "# def rows_to_df(table: Table) -> pd.DataFrame:\n",
    "#     return table.search(\" \").limit(999999).to_pandas()\n",
    "\n",
    "# sample_document = rows_to_df(document_table)\n",
    "# sample_document.text\n",
    "\n",
    "sample_document = pd.read_csv(r\"C:\\Users\\vtorr\\Work\\Projects\\aipatent\\evals\\GVHD-ALD-papers-not-false.csv\")\n",
    "sample_document.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Document(BaseModel):\n",
    "    title: str\n",
    "    filename: str\n",
    "    id: str\n",
    "    text: str\n",
    "\n",
    "sample_chunks = [\n",
    "    Document(\n",
    "        title=\"Lactose drives Enterococcus expansion to promote graft-versus-host disease\",\n",
    "        filename=row[\"filename\"],\n",
    "        id=str(row[\"chunk_id\"]),\n",
    "        text=row[\"text\"]\n",
    "    )\n",
    "    for _, row in sample_document.iterrows()\n",
    "]\n",
    "\n",
    "n_questions = 3\n",
    "\n",
    "example_questions = [\n",
    "    \"What is the composition of the substance?\",\n",
    "    \"How does the component operate, and what are his effects in the organism?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChunkEval(question='What targeting strategy is described for modulating gut bacteria in the paper?', answer='The paper describes a strategy where bacteriophages are used to specifically target and deplete gut bacteria, which in turn helps in attenuating alcoholic liver disease.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: What targeting strategy is described for modulating gut bacteria in the paper?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n'),\n",
       " ChunkEval(question='How does the bacteriophage intervention affect the progression of alcoholic liver disease?', answer='By targeting specific gut bacteria, the bacteriophage intervention reduces bacterial populations that contribute to disease progression, thereby attenuating alcoholic liver disease.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: How does the bacteriophage intervention affect the progression of alcoholic liver disease?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n'),\n",
       " ChunkEval(question='What therapeutic potential does bacteriophage targeting of gut bacteria offer according to the study?', answer='The study implies that precise bacteriophage targeting of gut bacteria can modulate the gut-liver axis, offering a potential therapeutic approach to mitigate alcoholic liver disease.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: What therapeutic potential does bacteriophage targeting of gut bacteria offer according to the study?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import List\n",
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class ChunkEval(QuestionAnswer):\n",
    "    chunk_id: str\n",
    "    question_with_context: str\n",
    "\n",
    "async def generate_evals(\n",
    "    document: Document, n_questions: int, example_questions: List[str]\n",
    ") -> List[ChunkEval]:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Generate `{n_questions}` question-answer pairs about {document.title}. The answers should primarily be derived from information in this piece of text from a microbiology paper:\n",
    "\n",
    "        <content>\n",
    "        {document.text}\n",
    "        </content>\n",
    "\n",
    "        Example questions:\n",
    "        {chr(10).join(f'- {q}' for q in example_questions)}\n",
    "\n",
    "        Provide a concise and specific answer for each question.\n",
    "        Do not use the exact example questions. Use them only as inspiration for the types of more specific questions to generate.\n",
    "        Do not include answers that are not in the content.\n",
    "        Questions should ask about biological characteristics and answers should refer to the content for clarification on the composition, effect or method of action of a given component or mechanism described in the piece of text.\n",
    "        Stylistically, the questions should resemble what would be asked to a RAG-based answer tool to access relevant information about target antigens and disease drivers to enrich a patent generation process.\n",
    "        \"\"\"\n",
    "\n",
    "    def make_context(question: str) -> str:\n",
    "        return f\"\"\"You receive the following question:\n",
    "Question: {question}\n",
    "This is about the following piece of text from a microbiology paper:\n",
    "Paper Title: {document.title}\n",
    "Paper chunk: {document.text}\n",
    "\"\"\"\n",
    "    try:\n",
    "        pairs = client.chat.completions.create_iterable(\n",
    "            model=\"o3-mini\",\n",
    "            response_model=QuestionAnswer,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            ChunkEval(\n",
    "                question=pair.question,\n",
    "                answer=pair.answer,\n",
    "                chunk_id=document.id,\n",
    "                question_with_context=make_context(pair.question),\n",
    "            )\n",
    "            async for pair in pairs\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating evals: {str(e)}\")\n",
    "        return []\n",
    "    \n",
    "first_chunk_res = await generate_evals(sample_chunks[0], n_questions, example_questions)\n",
    "first_chunk_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChunkEval(question='What component is used to target gut bacteria, and what impact does this have on the disease state according to the study?', answer='The study uses bacteriophage therapy to selectively target gut bacteria, which results in the attenuation of alcoholic liver disease.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: What component is used to target gut bacteria, and what impact does this have on the disease state according to the study?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n'),\n",
       " ChunkEval(question='How does the bacteriophage mechanism described in the paper modify gut bacterial populations?', answer='The bacteriophage specifically targets a gut bacterium, reducing its influence and thereby contributing to the attenuation of alcoholic liver disease.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: How does the bacteriophage mechanism described in the paper modify gut bacterial populations?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n'),\n",
       " ChunkEval(question='What role does bacteriophage play in the modulation of pathogenic processes in the context of the reported study?', answer='Bacteriophage is used to target gut bacteria, and this targeted approach leads to the attenuation of alcoholic liver disease by altering pathogenic processes.', chunk_id='2', question_with_context='You receive the following question:\\nQuestion: What role does bacteriophage play in the modulation of pathogenic processes in the context of the reported study?\\nThis is about the following piece of text from a microbiology paper:\\nPaper Title: Lactose drives Enterococcus expansion to promote graft-versus-host disease\\nPaper chunk: Bacteriophage targeting of gut bacterium attenuates alcoholic liver disease\\n')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "class ChunkProcessingError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "async def process_chunk(\n",
    "    document: Document,\n",
    "    n_questions: int,\n",
    "    example_questions: List[str],\n",
    "    semaphore: asyncio.Semaphore,\n",
    ") -> List[ChunkEval]:\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            return await generate_evals(document, n_questions, example_questions)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing chunk {document.id}: {str(e)}\")\n",
    "            raise ChunkProcessingError(f\"Failed to process chunk {document.id}\") from e\n",
    "\n",
    "\n",
    "# Test that we get the same results as directly calling generate_evals\n",
    "await process_chunk(\n",
    "    sample_chunks[0], n_questions, example_questions, asyncio.Semaphore(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID 257 is out of bounds.\n",
      "Chunk ID 253 is out of bounds.\n",
      "Chunk ID 264 is out of bounds.\n",
      "Chunk ID 254 is out of bounds.\n",
      "Chunk ID 238 is out of bounds.\n",
      "Chunk ID 270 is out of bounds.\n",
      "Chunk ID 225 is out of bounds.\n",
      "Chunk ID 368 is out of bounds.\n",
      "Chunk ID 375 is out of bounds.\n",
      "Chunk ID 276 is out of bounds.\n",
      "Chunk ID 222 is out of bounds.\n",
      "Chunk ID 264 is out of bounds.\n",
      "Chunk ID 217 is out of bounds.\n",
      "Chunk ID 700 is out of bounds.\n",
      "Chunk ID 244 is out of bounds.\n",
      "Chunk ID 689 is out of bounds.\n",
      "Chunk ID 514 is out of bounds.\n",
      "Chunk ID 240 is out of bounds.\n",
      "Chunk ID 228 is out of bounds.\n",
      "Chunk ID 214 is out of bounds.\n",
      "Chunk ID 673 is out of bounds.\n",
      "Chunk ID 693 is out of bounds.\n",
      "Chunk ID 463 is out of bounds.\n",
      "Chunk ID 229 is out of bounds.\n",
      "Chunk ID 658 is out of bounds.\n",
      "Chunk ID 501 is out of bounds.\n",
      "Chunk ID 425 is out of bounds.\n",
      "Chunk ID 700 is out of bounds.\n",
      "Chunk ID 322 is out of bounds.\n",
      "Chunk ID 697 is out of bounds.\n",
      "Chunk ID 484 is out of bounds.\n",
      "Chunk ID 261 is out of bounds.\n",
      "Chunk ID 230 is out of bounds.\n",
      "Chunk ID 292 is out of bounds.\n",
      "Chunk ID 451 is out of bounds.\n",
      "Chunk ID 692 is out of bounds.\n",
      "Chunk ID 410 is out of bounds.\n",
      "Chunk ID 547 is out of bounds.\n",
      "Chunk ID 227 is out of bounds.\n",
      "Chunk ID 563 is out of bounds.\n",
      "Chunk ID 396 is out of bounds.\n",
      "Chunk ID 241 is out of bounds.\n",
      "Chunk ID 481 is out of bounds.\n",
      "Chunk ID 298 is out of bounds.\n",
      "Chunk ID 543 is out of bounds.\n",
      "Chunk ID 267 is out of bounds.\n",
      "Chunk ID 206 is out of bounds.\n",
      "Chunk ID 490 is out of bounds.\n",
      "Chunk ID 692 is out of bounds.\n",
      "Chunk ID 691 is out of bounds.\n",
      "Chunk ID 699 is out of bounds.\n",
      "Chunk ID 397 is out of bounds.\n",
      "Chunk ID 708 is out of bounds.\n",
      "Chunk ID 509 is out of bounds.\n",
      "Chunk ID 577 is out of bounds.\n",
      "Chunk ID 691 is out of bounds.\n",
      "Chunk ID 707 is out of bounds.\n",
      "Chunk ID 550 is out of bounds.\n",
      "Chunk ID 220 is out of bounds.\n",
      "Chunk ID 514 is out of bounds.\n",
      "Chunk ID 308 is out of bounds.\n",
      "Chunk ID 371 is out of bounds.\n",
      "Chunk ID 651 is out of bounds.\n",
      "Chunk ID 215 is out of bounds.\n",
      "Chunk ID 425 is out of bounds.\n",
      "Chunk ID 224 is out of bounds.\n",
      "Chunk ID 410 is out of bounds.\n",
      "Chunk ID 466 is out of bounds.\n",
      "Chunk ID 262 is out of bounds.\n",
      "Chunk ID 451 is out of bounds.\n",
      "Chunk ID 706 is out of bounds.\n",
      "Chunk ID 546 is out of bounds.\n",
      "Chunk ID 492 is out of bounds.\n",
      "Chunk ID 561 is out of bounds.\n",
      "Chunk ID 234 is out of bounds.\n",
      "Chunk ID 227 is out of bounds.\n",
      "Chunk ID 515 is out of bounds.\n",
      "Chunk ID 501 is out of bounds.\n",
      "Chunk ID 370 is out of bounds.\n",
      "Chunk ID 278 is out of bounds.\n",
      "Chunk ID 577 is out of bounds.\n",
      "Chunk ID 694 is out of bounds.\n",
      "Chunk ID 658 is out of bounds.\n",
      "Chunk ID 238 is out of bounds.\n",
      "Chunk ID 209 is out of bounds.\n",
      "Chunk ID 258 is out of bounds.\n",
      "Chunk ID 245 is out of bounds.\n",
      "Chunk ID 414 is out of bounds.\n",
      "Chunk ID 245 is out of bounds.\n",
      "Chunk ID 702 is out of bounds.\n",
      "Chunk ID 412 is out of bounds.\n",
      "Chunk ID 447 is out of bounds.\n",
      "Chunk ID 725 is out of bounds.\n",
      "Chunk ID 530 is out of bounds.\n",
      "Chunk ID 214 is out of bounds.\n",
      "Chunk ID 277 is out of bounds.\n",
      "Chunk ID 278 is out of bounds.\n",
      "Chunk ID 484 is out of bounds.\n",
      "Chunk ID 273 is out of bounds.\n",
      "Chunk ID 240 is out of bounds.\n",
      "Chunk ID 263 is out of bounds.\n",
      "Chunk ID 664 is out of bounds.\n",
      "Chunk ID 728 is out of bounds.\n",
      "Chunk ID 262 is out of bounds.\n",
      "Chunk ID 607 is out of bounds.\n",
      "Chunk ID 298 is out of bounds.\n",
      "Chunk ID 247 is out of bounds.\n",
      "Chunk ID 549 is out of bounds.\n",
      "Chunk ID 540 is out of bounds.\n",
      "Chunk ID 264 is out of bounds.\n",
      "Chunk ID 520 is out of bounds.\n",
      "Chunk ID 702 is out of bounds.\n",
      "Chunk ID 229 is out of bounds.\n",
      "Chunk ID 509 is out of bounds.\n",
      "Chunk ID 277 is out of bounds.\n",
      "Chunk ID 263 is out of bounds.\n",
      "Chunk ID 424 is out of bounds.\n",
      "Chunk ID 283 is out of bounds.\n",
      "Chunk ID 410 is out of bounds.\n",
      "Chunk ID 515 is out of bounds.\n",
      "Chunk ID 597 is out of bounds.\n",
      "Chunk ID 266 is out of bounds.\n",
      "Chunk ID 561 is out of bounds.\n",
      "Chunk ID 704 is out of bounds.\n",
      "Chunk ID 217 is out of bounds.\n",
      "Chunk ID 697 is out of bounds.\n",
      "Chunk ID 542 is out of bounds.\n",
      "Chunk ID 746 is out of bounds.\n",
      "Chunk ID 491 is out of bounds.\n",
      "Chunk ID 261 is out of bounds.\n",
      "Chunk ID 396 is out of bounds.\n",
      "Chunk ID 689 is out of bounds.\n",
      "Chunk ID 223 is out of bounds.\n",
      "Chunk ID 244 is out of bounds.\n",
      "Chunk ID 255 is out of bounds.\n",
      "Chunk ID 740 is out of bounds.\n",
      "Chunk ID 490 is out of bounds.\n",
      "Chunk ID 691 is out of bounds.\n",
      "Chunk ID 567 is out of bounds.\n",
      "Chunk ID 252 is out of bounds.\n",
      "Chunk ID 260 is out of bounds.\n",
      "Chunk ID 262 is out of bounds.\n",
      "Chunk ID 499 is out of bounds.\n",
      "Chunk ID 514 is out of bounds.\n",
      "Chunk ID 252 is out of bounds.\n",
      "Chunk ID 254 is out of bounds.\n",
      "Chunk ID 543 is out of bounds.\n",
      "Chunk ID 224 is out of bounds.\n",
      "Chunk ID 220 is out of bounds.\n",
      "Chunk ID 245 is out of bounds.\n",
      "Chunk ID 562 is out of bounds.\n",
      "Chunk ID 725 is out of bounds.\n",
      "Chunk ID 619 is out of bounds.\n",
      "Chunk ID 237 is out of bounds.\n",
      "Chunk ID 542 is out of bounds.\n",
      "Chunk ID 621 is out of bounds.\n",
      "Chunk ID 563 is out of bounds.\n",
      "Chunk ID 447 is out of bounds.\n",
      "Chunk ID 562 is out of bounds.\n",
      "Chunk ID 549 is out of bounds.\n",
      "Chunk ID 745 is out of bounds.\n",
      "Chunk ID 567 is out of bounds.\n",
      "Chunk ID 708 is out of bounds.\n",
      "Chunk ID 244 is out of bounds.\n",
      "Chunk ID 530 is out of bounds.\n",
      "Chunk ID 214 is out of bounds.\n",
      "Chunk ID 256 is out of bounds.\n",
      "Chunk ID 472 is out of bounds.\n",
      "Chunk ID 209 is out of bounds.\n",
      "Chunk ID 228 is out of bounds.\n",
      "Chunk ID 447 is out of bounds.\n",
      "Chunk ID 277 is out of bounds.\n",
      "Chunk ID 252 is out of bounds.\n",
      "Chunk ID 259 is out of bounds.\n",
      "Chunk ID 398 is out of bounds.\n",
      "Chunk ID 368 is out of bounds.\n",
      "Chunk ID 301 is out of bounds.\n",
      "Chunk ID 276 is out of bounds.\n",
      "Chunk ID 314 is out of bounds.\n",
      "Chunk ID 238 is out of bounds.\n",
      "Chunk ID 704 is out of bounds.\n",
      "Chunk ID 664 is out of bounds.\n",
      "Chunk ID 219 is out of bounds.\n",
      "Chunk ID 567 is out of bounds.\n",
      "Chunk ID 207 is out of bounds.\n",
      "Chunk ID 212 is out of bounds.\n",
      "Chunk ID 258 is out of bounds.\n",
      "Chunk ID 597 is out of bounds.\n",
      "Chunk ID 550 is out of bounds.\n",
      "Chunk ID 466 is out of bounds.\n",
      "Chunk ID 690 is out of bounds.\n",
      "Chunk ID 690 is out of bounds.\n",
      "Chunk ID 375 is out of bounds.\n",
      "Chunk ID 207 is out of bounds.\n",
      "Chunk ID 696 is out of bounds.\n",
      "Chunk ID 566 is out of bounds.\n",
      "Chunk ID 414 is out of bounds.\n",
      "Chunk ID 479 is out of bounds.\n",
      "Chunk ID 273 is out of bounds.\n",
      "Chunk ID 479 is out of bounds.\n",
      "Chunk ID 225 is out of bounds.\n",
      "Chunk ID 539 is out of bounds.\n",
      "Chunk ID 314 is out of bounds.\n",
      "Chunk ID 280 is out of bounds.\n",
      "Chunk ID 249 is out of bounds.\n",
      "Chunk ID 481 is out of bounds.\n",
      "Chunk ID 490 is out of bounds.\n",
      "Chunk ID 566 is out of bounds.\n",
      "Chunk ID 227 is out of bounds.\n",
      "Chunk ID 397 is out of bounds.\n",
      "Chunk ID 607 is out of bounds.\n",
      "Chunk ID 527 is out of bounds.\n",
      "Chunk ID 544 is out of bounds.\n",
      "Chunk ID 692 is out of bounds.\n",
      "Chunk ID 527 is out of bounds.\n",
      "Chunk ID 472 is out of bounds.\n",
      "Chunk ID 260 is out of bounds.\n",
      "Chunk ID 577 is out of bounds.\n",
      "Chunk ID 208 is out of bounds.\n",
      "Chunk ID 258 is out of bounds.\n",
      "Chunk ID 730 is out of bounds.\n",
      "Chunk ID 540 is out of bounds.\n",
      "Chunk ID 673 is out of bounds.\n",
      "Chunk ID 281 is out of bounds.\n",
      "Chunk ID 491 is out of bounds.\n",
      "Chunk ID 220 is out of bounds.\n",
      "Generated 612 ChunkEvals.\n",
      "Saved 306 samples as 'synthetic_eval_cleaned_gvhd_ald.json'\n",
      "Saved 306 samples as 'synthetic_finetune_cleaned_gvhd_ald.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "async def create_synthetic_dataset(\n",
    "    documents: List[Document],\n",
    "    n_questions: int,\n",
    "    example_questions: List[str],\n",
    "    max_concurrency: int = 10,\n",
    ") -> List[ChunkEval]:\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "    tasks = [\n",
    "        process_chunk(document, n_questions, example_questions, semaphore)\n",
    "        for document in documents\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    dataset = []\n",
    "    for result in results:\n",
    "        if isinstance(result, ChunkProcessingError):\n",
    "            print(result)\n",
    "        elif isinstance(result, list):\n",
    "            dataset.extend(result)\n",
    "        else:\n",
    "            print(f\"Unexpected result type: {type(result)}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_eval_data(dataset: List[ChunkEval], filename: str):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump([chunk_eval.model_dump() for chunk_eval in dataset], f, indent=2)\n",
    "\n",
    "def save_ft_data(dataset: List[ChunkEval], filename: str):\n",
    "    documents_df = pd.read_csv(r\"C:\\Users\\vtorr\\Work\\Projects\\aipatent\\evals\\GVHD-ALD-papers-not-false.csv\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        for chunk_eval in dataset:\n",
    "            if int(chunk_eval.chunk_id) < len(documents_df):\n",
    "                document_text = documents_df.iloc[int(chunk_eval.chunk_id)].text\n",
    "                json_object = {\n",
    "                    'query': chunk_eval.question_with_context,\n",
    "                    'relevant_passages': [document_text],\n",
    "                }\n",
    "                f.write(json.dumps(json_object) + '\\n')\n",
    "            else:\n",
    "                print(f\"Chunk ID {chunk_eval.chunk_id} is out of bounds.\")\n",
    "\n",
    "synthetic_dataset = await create_synthetic_dataset(\n",
    "    sample_chunks, n_questions, example_questions\n",
    ")\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "random.shuffle(synthetic_dataset)\n",
    "\n",
    "# Split the dataset into two halves\n",
    "split_index = len(synthetic_dataset) // 2\n",
    "eval_dataset = synthetic_dataset[:split_index]\n",
    "finetune_dataset = synthetic_dataset[split_index:]\n",
    "\n",
    "# Save the two datasets\n",
    "save_eval_data(eval_dataset, \"synthetic_eval_cleaned_gvhd_ald.json\")\n",
    "save_ft_data(finetune_dataset, \"synthetic_finetune_cleaned_gvhd_ald.jsonl\")\n",
    "\n",
    "print(f\"Generated {len(synthetic_dataset)} ChunkEvals.\")\n",
    "print(f\"Saved {len(eval_dataset)} samples as 'synthetic_eval_cleaned_gvhd_ald.json'\")\n",
    "print(f\"Saved {len(finetune_dataset)} samples as 'synthetic_finetune_cleaned_gvhd_ald.jsonl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> 5b6e3e1f6bb904635df1f05e870b8aeeed94cf1b
